var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = GroupIWord2Vec","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [GroupIWord2Vec]","category":"page"},{"location":"#GroupIWord2Vec.GroupIWord2Vec","page":"Home","title":"GroupIWord2Vec.GroupIWord2Vec","text":"GroupIWord2Vec\n\nThis is the main module file that organizes all the word embedding functionality.\n\nTypes\n\nWordEmbedding: Main data structure for word embeddings\n\nFunctions\n\ntrain_model: Train new word embeddings\nload_embeddings: Load pre-trained embeddings\nget_vector: Get a word's vector\nget_similarity: Find top n similar words\ncosine_similarity: Compute similarity of two words\n\n\n\n\n\n","category":"module"},{"location":"#GroupIWord2Vec.cosine_similarity-Tuple{WordEmbedding, Any, Any}","page":"Home","title":"GroupIWord2Vec.cosine_similarity","text":"cosine_similarity(wv, string_1, string_2)\n\nPurpose: Return the cosine similarity value between two words\n\n\n\n\n\n","category":"method"},{"location":"#GroupIWord2Vec.get_top_similarity_of_vector","page":"Home","title":"GroupIWord2Vec.get_top_similarity_of_vector","text":"get_top_similarity_of_vector(wv, vector, int)\n\nPurpose: Find the n (default n = 10) most similar words to a given vector and return the matching strings\n\n\n\n\n\n","category":"function"},{"location":"#GroupIWord2Vec.get_top_similarity_of_word","page":"Home","title":"GroupIWord2Vec.get_top_similarity_of_word","text":"get_top_similarity_of_word(wv, string, int)\n\nPurpose: Find the n (default n = 10) most similar words to a given word and return the matching strings\n\n\n\n\n\n","category":"function"},{"location":"#GroupIWord2Vec.get_vector_from_word-Tuple{WordEmbedding, Any}","page":"Home","title":"GroupIWord2Vec.get_vector_from_word","text":"get_vector_from_word(wv, string)\n\nPurpose: Get the vector representation of an input word from the WordEmbedding\n\n\n\n\n\n","category":"method"},{"location":"#GroupIWord2Vec.read_binary_format-Union{Tuple{T}, Tuple{AbstractString, Type{T}, Bool, Char, Int64}} where T<:Real","page":"Home","title":"GroupIWord2Vec.read_binary_format","text":"This function reads word embeddings (word->vector mappings) from a binary file\n\nIt requires the following Parameters:\n\nfilepath: where the file is located\n\nT: what kind of numbers we want (like decimal numbers)\n\nnormalize: whether to make all vectors have length 1\n\n–-> This can be useful for comparison since the length of the vector does not\n\nmatter, only its direction\n\nseparator: what character separates the values in the file (like space or comma)\n\nskip_bytes: how many bytes to skip after each word-vector pair (usually for handling separators)\n\nInstead of reading lines of text and parsing numbers it reads words until it hits a separator\n\nReads raw bytes and converts them directly to numbers\n\n\n\n\n\n","category":"method"},{"location":"#GroupIWord2Vec.read_text_format-Union{Tuple{T}, Tuple{AbstractString, Type{T}, Bool, Char}} where T<:Real","page":"Home","title":"GroupIWord2Vec.read_text_format","text":"This function reads word embeddings (word->vector mappings) from a text file\n\nIt requires the following Parameters:\n\nfilepath: where the file is located\n\nT: what kind of numbers we want (like decimal numbers)\n\nnormalize: whether to make all vectors have length 1\n\n–-> This can be useful for comparison since the length of the vector does not\n\nmatter, only its direction\n\nseparator: what character separates the values in the file (like space or comma)\n\n\n\n\n\n","category":"method"},{"location":"#GroupIWord2Vec.train_model-Tuple{AbstractString, AbstractString}","page":"Home","title":"GroupIWord2Vec.train_model","text":" word2vec(train, output; size=100, window=5, sample=1e-3, hs=0,  negative=5, threads=12, iter=5, min_count=5, alpha=0.025, debug=2, binary=1, cbow=1, save_vocal=Nothing(), read_vocab=Nothing(), verbose=false,)\n\nParameters for training:\n    train <file>\n        Use text data from <file> to train the model\n    output <file>\n        Use <file> to save the resulting word vectors / word clusters\n    size <Int>\n        Set size of word vectors; default is 100\n    window <Int>\n        Set max skip length between words; default is 5\n    sample <AbstractFloat>\n        Set threshold for occurrence of words. Those that appear with\n        higher frequency in the training data will be randomly\n        down-sampled; default is 1e-5.\n    hs <Int>\n        Use Hierarchical Softmax; default is 1 (0 = not used)\n    negative <Int>\n        Number of negative examples; default is 0, common values are \n        5 - 10 (0 = not used)\n    threads <Int>\n        Use <Int> threads (default 12)\n    iter <Int>\n        Run more training iterations (default 5)\n    min_count <Int>\n        This will discard words that appear less than <Int> times; default\n        is 5\n    alpha <AbstractFloat>\n        Set the starting learning rate; default is 0.025\n    debug <Int>\n        Set the debug mode (default = 2 = more info during training)\n    binary <Int>\n        Save the resulting vectors in binary moded; default is 0 (off)\n    cbow <Int>\n        Use the continuous back of words model; default is 1 (skip-gram\n        model)\n    save_vocab <file>\n        The vocabulary will be saved to <file>\n    read_vocab <file>\n        The vocabulary will be read from <file>, not constructed from the\n        training data\n    verbose <Bool>\n        Print output from training\n\n\n\n\n\n","category":"method"},{"location":"#GroupIWord2Vec.word_analogy","page":"Home","title":"GroupIWord2Vec.word_analogy","text":"word_analogy(wv::WordEmbedding, pos_words::Vector{String}, neg_words::Vector{String}, n::Int=5)\n\nPerforms word analogy calculations like: king - man + woman = queen Returns the n most similar words to the resulting vector.\n\nArguments\n\nwv: WordEmbedding containing the vocabulary and vectors\npos_words: Words to add to the calculation\nneg_words: Words to subtract from the calculation\nn: Number of similar words to return (default: 5)\n\nReturns\n\nVector{String}: n most similar words to the resulting vector\n\nExample\n\n# Find: king - man + woman = ?\nresult = word_analogy(wv, [\"king\", \"woman\"], [\"man\"])\n# Should return [\"queen\", ...]\n\n\n\n\n\n","category":"function"}]
}
