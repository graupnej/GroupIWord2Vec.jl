var documenterSearchIndex = {"docs":
[{"location":"functions/","page":"All Functions List","title":"All Functions List","text":"Modules = [GroupIWord2Vec]","category":"page"},{"location":"functions/#GroupIWord2Vec.WordEmbedding","page":"All Functions List","title":"GroupIWord2Vec.WordEmbedding","text":"WordEmbedding{S<:AbstractString, T<:Real}\n\nA structure for storing and managing word embeddings, where each word is associated with a vector representation.\n\nFields\n\nwords::Vector{S}: List of all words in the vocabulary\nembeddings::Matrix{T}: Matrix where each column is a word's vector representation\nword_indices::Dict{S, Int}: Dictionary mapping words to their positions in the vocabulary\n\nType Parameters\n\nS: Type of strings used (defaults to String)\nT: Type of numbers in the embedding vectors (defaults to Float64)\n\nConstructor\n\nWordEmbedding(words::Vector{S}, matrix::Matrix{T}) where {S<:AbstractString, T<:Real}\n\nCreates a WordEmbedding with the given vocabulary and corresponding vectors.\n\nArguments\n\nwords::Vector{S}: Vector of words in the vocabulary\nmatrix::Matrix{T}: Matrix where each column corresponds to one word's vector\n\nThrows\n\nArgumentError: If the number of words doesn't match the number of vectors (matrix columns)\n\nExample\n\n```julia\n\nCreate a simple word embedding with 2D vectors\n\nwords = [\"cat\", \"dog\", \"house\"] vectors = [0.5 0.1 0.8;           0.2 0.9 0.3] embedding = WordEmbedding(words, vectors)\n\n\n\n\n\n","category":"type"},{"location":"functions/#GroupIWord2Vec.get_any2vec-Tuple{WordEmbedding, Union{String, Vector{Float64}}}","page":"All Functions List","title":"GroupIWord2Vec.get_any2vec","text":"get_any2vec(wv::WordEmbedding, word_or_vec::Union{String, Vector{Float64}}) -> Vector{Float64}\n\nConverts a word into its corresponding vector representation or returns the vector unchanged if already provided\n\nArguments\n\nwv::WordEmbedding: The word embedding structure containing the vocabulary and embeddings\nword_or_vec::Union{String, Vector{Float64}}: A word to be converted into a vector, or a numerical vector to be validated\n\nReturns\n\nVector{Float64}: The vector representation of the word if input is a String, or the validated vector\n\nThrows\n\nDimensionMismatch: If the input vector does not match the embedding dimension.\nArgumentError: If the input is neither a word nor a valid numeric vector.\n\nExample\n\nwords = [\"cat\", \"dog\"]\nvectors = [0.5 0.1;\n          0.2 0.9]\nwv = WordEmbedding(words, vectors)\n\nget_any2vec(wv, \"cat\")  # Returns [0.5, 0.2]\nget_any2vec(wv, [0.5, 0.2])  # Returns [0.5, 0.2]\n\n\n\n\n\n","category":"method"},{"location":"functions/#GroupIWord2Vec.get_similar_words","page":"All Functions List","title":"GroupIWord2Vec.get_similar_words","text":"get_similar_words(wv::WordEmbedding, word_or_vec::Union{AbstractString, AbstractVector{<:Real}}, n::Int=10) -> Vector{String}\n\nFinds the n most similar words to a given word or vector based on cosine similarity.\n\nArguments\n\nwv: The word embedding model.\nword_or_vec: The target word or embedding vector.\nn: Number of similar words to return (default: 10).\n\nThrows\n\nArgumentError: If n is not positive, the word is missing, or the vector has zero norm.\nDimensionMismatch: If the vector size is incorrect.\n\nReturns\n\nA list of n most similar words, sorted by similarity.\n\nExample\n\nget_similar_words(model, \"cat\", 5)  # [\"dog\", \"kitten\", \"feline\", \"puppy\", \"pet\"]\nget_similar_words(model, get_word2vec(model, \"ocean\"), 3)  # [\"sea\", \"water\", \"wave\"]\n\n\n\n\n\n","category":"function"},{"location":"functions/#GroupIWord2Vec.get_vec2word-Tuple{WordEmbedding, Vector{Float64}}","page":"All Functions List","title":"GroupIWord2Vec.get_vec2word","text":"get_vec2word(wv::WordEmbedding,vec::Vector{Float64}) -> String\n\nRetrieves the closest word in the embedding space to a given vector based on cosine similarity.\n\nArguments\n\nwv::WordEmbedding: The word embedding structure containing the vocabulary and embeddings\nvec::Vector{Float64}: A vector representation of a word\n\nReturns\n\nString: The word from the vocabulary closest to the given vector\n\nThrows\n\nDimensionMismatch: If the input vector's dimension does not match the word vector dimensions\n\nExample\n\nwords = [\"cat\", \"dog\"]\nvectors = [0.5 0.1;\n          0.2 0.9]\nembedding = WordEmbedding(words, vectors)\n\nget_vec2word(embedding, [0.51, 0.19])  # Returns \"cat\"\n\n\n\n\n\n","category":"method"},{"location":"functions/#GroupIWord2Vec.get_vector_operation-Tuple{WordEmbedding, Union{String, Vector{Float64}}, Union{String, Vector{Float64}}, Symbol}","page":"All Functions List","title":"GroupIWord2Vec.get_vector_operation","text":"get_vector_operation(ww::WordEmbedding, inp1::Union{String, Vector{Float64}}, inp2::Union{String, Vector{Float64}}, operator::Symbol) -> Union{Vector{Float64}, Float64}\n\nPerforms a mathematical operation between two word embedding vectors\n\nArguments\n\nww::WordEmbedding: The word embedding structure containing the vocabulary and embeddings\ninp1::Union{String, Vector{Float64}}: The first input, which can be a word (String) or a precomputed embedding vector\ninp2::Union{String, Vector{Float64}}: The second input, which can be a word (String) or a precomputed embedding vector\noperator::Symbol: The operation to perform. Must be one of :+, :-, :cosine, or :euclid\n\nThrows\n\nArgumentError: If the operator is invalid.\nArgumentError: If cosine similarity is attempted on a zero vector\nDimensionMismatch: If the input vectors do not have the same length\n\nReturns\n\nVector{Float64}: If the operation is :+ or :-, returns the resulting word vector\nFloat64: If the operation is :cosine or :euclid, returns a scalar value\n\nExample\n\nvec = get_vector_operation(model, \"king\", \"man\", :-)\nsimilarity = get_vector_operation(model, \"cat\", \"dog\", :cosine)\ndistance = get_vector_operation(model, \"car\", \"bicycle\", :euclid)\n\n\n\n\n\n","category":"method"},{"location":"functions/#GroupIWord2Vec.get_word2vec-Tuple{WordEmbedding, String}","page":"All Functions List","title":"GroupIWord2Vec.get_word2vec","text":"get_word2vec(wv::WordEmbedding, word::String) -> Vector{Float64}\n\nRetrieves the embedding vector corresponding to a given word.\n\nArguments\n\nwv::WordEmbedding: The word embedding structure containing the vocabulary and embeddings\nword::String: The word to look up\n\nThrows\n\nArgumentError: If the word is not found in the embedding model\n\nReturns\n\nVector{Float64}: The embedding vector of the requested word of type Float64\n\nExample\n\nvec = get_word2vec(model, \"dog\")\n\n\n\n\n\n","category":"method"},{"location":"functions/#GroupIWord2Vec.get_word_analogy","page":"All Functions List","title":"GroupIWord2Vec.get_word_analogy","text":"get_word_analogy(wv::WordEmbedding, inp1::T, inp2::T, inp3::T, n::Int=5) where {T<:Union{AbstractString, AbstractVector{<:Real}}} -> Vector{String}\n\nFinds the top n words that best complete the analogy: inp1 - inp2 + inp3 = ?.\n\nArguments\n\nwv::WordEmbedding: The word embedding model.\ninp1, inp2, inp3::T: Words or vectors for analogy computation.\nn::Int=5: Number of closest matching words to return.\n\nReturns\n\nVector{String}: A list of the top n matching words.\n\nNotes\n\nInput words are converted to vectors automatically.\nThe computed analogy vector is normalized.\nInput words (if given as strings) are excluded from results.\n\nExample\n\nget_word_analogy(model, \"king\", \"man\", \"woman\", 3) \n# → [\"queen\", \"princess\", \"duchess\"]\n\n\n\n\n\n","category":"function"},{"location":"functions/#GroupIWord2Vec.load_embeddings-Tuple{String}","page":"All Functions List","title":"GroupIWord2Vec.load_embeddings","text":"load_embeddings(path::AbstractString; format::Union{:text, :binary}=:text, \n                data_type::Type{T}=Float64, normalize_vectors::Bool=true, \n                separator::Char=' ', skip_bytes::Int=0) -> WordEmbedding\n\nLoads word embeddings from a text or binary file.\n\nArguments\n\npath::AbstractString: Path to the embedding file.\nformat::Union{:text, :binary}=:text: File format (:text or :binary).\ndata_type::Type{T}=Float64: Type of word vectors (Float32, Float64, etc.).\nnormalize_vectors::Bool=true: Normalize vectors to unit length.\nseparator::Char=' ': Word-vector separator in text files.\nskip_bytes::Int=0: Bytes to skip after each word-vector pair in binary files.\n\nThrows\n\nArgumentError: If format is not :text or :binary.\n\nReturns\n\nWordEmbedding{S, T}: The loaded word embeddings.\n\nExample\n\n```julia embedding = loadembeddings(\"vectors.txt\")  # Load text format embedding = loadembeddings(\"vectors.bin\", format=:binary, datatype=Float32, skipbytes=1)  # Load binary format\n\n\n\n\n\n","category":"method"},{"location":"functions/#GroupIWord2Vec.read_binary_format-Union{Tuple{T}, Tuple{AbstractString, Type{T}, Bool, Char, Int64}} where T<:Real","page":"All Functions List","title":"GroupIWord2Vec.read_binary_format","text":"This function reads word embeddings (word->vector mappings) from a binary file\n\nIt requires the following Parameters:\n\nfilepath: where the file is located\n\nT: what kind of numbers we want (like decimal numbers)\n\nnormalize: whether to make all vectors have length 1\n\n–-> This can be useful for comparison since the length of the vector does not\n\nmatter, only its direction\n\nseparator: what character separates the values in the file (like space or comma)\n\nskip_bytes: how many bytes to skip after each word-vector pair (usually for handling separators)\n\nInstead of reading lines of text and parsing numbers it reads words until it hits a separator\n\nReads raw bytes and converts them directly to numbers\n\n\n\n\n\n","category":"method"},{"location":"functions/#GroupIWord2Vec.read_text_format-Union{Tuple{T}, Tuple{AbstractString, Type{T}, Bool, Char}} where T<:Real","page":"All Functions List","title":"GroupIWord2Vec.read_text_format","text":"This function reads word embeddings (word->vector mappings) from a text file\n\nIt requires the following Parameters:\n\nfilepath: where the file is located\n\nT: what kind of numbers we want (like decimal numbers)\n\nnormalize: whether to make all vectors have length 1\n\n–-> This can be useful for comparison since the length of the vector does not\n\nmatter, only its direction\n\nseparator: what character separates the values in the file (like space or comma)\n\n\n\n\n\n","category":"method"},{"location":"functions/#GroupIWord2Vec.reduce_to_2d","page":"All Functions List","title":"GroupIWord2Vec.reduce_to_2d","text":"reduce_to_2d(data::Matrix{Float64}, number_of_pc::Int=2) -> Matrix{Float64}\n\nPerforms Principal Component Analysis (PCA) to reduce the dimensionality of a given dataset and returns a projected data\n\nArguments\n\ndata::Matrix{Float64}: The input data matrix where rows represent samples and columns represent features.\nnumber_of_pc::Int=2: The number of principal components to retain (default: 2).\n\nReturns\n\nMatrix{Float64}: A matrix of shape (number_of_pc × N), where N is the number of samples, containing the projected data in the reduced dimensional space.\n\nExample\n\ndata = randn(100, 50)  # 100 samples, 50 features\nreduced_data = reduce_to_2d(data, 2)\n\n\n\n\n\n","category":"function"},{"location":"functions/#GroupIWord2Vec.show_relations-Tuple{Vararg{String}}","page":"All Functions List","title":"GroupIWord2Vec.show_relations","text":"show_relations(words::String...; wv::WordEmbedding, save_path::String=\"word_relations.png\") -> Plots.Plot\n\nGenerates a 2D PCA projection of the given word embeddings and visualizes their relationships like this: arg1==>arg2, arg3==>arg4, ... Note: Use an even number of inputs!\n\nArguments\n\nwords::String...: A list of words to visualize. The number of words must be a multiple of 2.\nwv::WordEmbedding: The word embedding structure containing the word vectors.\nsave_path::String=\"word_relations.png\": The file path for the generated plot. Not saved if empty or nothing\n\nThrows\n\nArgumentError: If the number of words is not a multiple of 2.\nArgumentError: If any of the provided words are not found in the embedding model.\n\nReturns\n\nPlots.Plot: A scatter plot with arrows representing word relationships.\n\nExample\n\np = show_relations(\"king\", \"queen\", \"man\", \"woman\"; wv=model, save_path=\"relations.png\")\n\n\n\n\n\n","category":"method"},{"location":"functions/#GroupIWord2Vec.train_model-Tuple{AbstractString, AbstractString}","page":"All Functions List","title":"GroupIWord2Vec.train_model","text":" word2vec(train, output; size=100, window=5, sample=1e-3, hs=0,  negative=5, threads=12, iter=5, min_count=5, alpha=0.025, debug=2, binary=1, cbow=1, save_vocal=Nothing(), read_vocab=Nothing(), verbose=false,)\n\nParameters for training:\n    train <file>\n        Use text data from <file> to train the model\n    output <file>\n        Use <file> to save the resulting word vectors / word clusters\n    size <Int>\n        Set size of word vectors; default is 100\n    window <Int>\n        Set max skip length between words; default is 5\n    sample <AbstractFloat>\n        Set threshold for occurrence of words. Those that appear with\n        higher frequency in the training data will be randomly\n        down-sampled; default is 1e-5.\n    hs <Int>\n        Use Hierarchical Softmax; default is 1 (0 = not used)\n    negative <Int>\n        Number of negative examples; default is 0, common values are \n        5 - 10 (0 = not used)\n    threads <Int>\n        Use <Int> threads (default 12)\n    iter <Int>\n        Run more training iterations (default 5)\n    min_count <Int>\n        This will discard words that appear less than <Int> times; default\n        is 5\n    alpha <AbstractFloat>\n        Set the starting learning rate; default is 0.025\n    debug <Int>\n        Set the debug mode (default = 2 = more info during training)\n    binary <Int>\n        Save the resulting vectors in binary moded; default is 0 (off)\n    cbow <Int>\n        Use the continuous back of words model; default is 1 (skip-gram\n        model)\n    save_vocab <file>\n        The vocabulary will be saved to <file>\n    read_vocab <file>\n        The vocabulary will be read from <file>, not constructed from the\n        training data\n    verbose <Bool>\n        Print output from training\n\n\n\n\n\n","category":"method"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"(Image: Coverage) (Image: Build Status) (Image: Dev)","category":"page"},{"location":"getting_started/#Word-Embeddings","page":"Getting Started","title":"Word Embeddings","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Word Embeddings are numerical representations of words in a high-dimensional vector space, where words with similar meanings are positioned closer together. These vectors capture semantic relationships between words, allowing machines to understand language context and meaning through mathematical operations. They serve as the foundation for many natural language processing tasks. This package allows training an ML model to create word embeddings based on a source text and provides functionality to work with the generated word embedding vectors.","category":"page"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"This is an overview of the project's main file structure:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"GroupIWord2Vec.jl               \n├── src/                        # Contains core modules for the package\n│   ├── GroupIWord2Vec.jl       # Main entry point for the project\n│   ├── functions.jl            # Word/vector functions\n│   └── model.jl                # Model functions\n├── test/                       # Unit tests to validate functionalities\n│   ├── runtests.jl             # Combination of every testing routine\n│   ├── test_functions.jl       # Testing routine for word/vector functions \n│   └── test_model.jl           # Testing routine for model functions\n├── docs/                       # Documentation for the package\n├── Manifest.toml               # Detailed dependency lock file that tracks exact versions of project dependencies\n├── Project.toml                # Project configuration file defining package dependencies\n└── README.md                   # Main documentation file containing getting started","category":"page"},{"location":"getting_started/#Download","page":"Getting Started","title":"Download","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Pluto's built-in environments cannot be used, so to start a custom environment must be created and managed manually.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> using Pkg\njulia> Pkg.activate(\"MyEnv\")\njulia> Pkg.add(url=\"https://github.com/graupnej/GroupIWord2Vec.jl\")\njulia> using GroupIWord2Vec","category":"page"},{"location":"getting_started/#Examples","page":"Getting Started","title":"Examples","text":"","category":"section"},{"location":"getting_started/#Train-Model-and-Create-Word-Embeddings-Text8","page":"Getting Started","title":"Train Model and Create Word Embeddings - Text8","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Download the text corpus text8 and store it in the current working directory. To train the model with this text corpus use train_model()","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> train_model(\"text8\", \"text8.txt\", verbose = true)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The resulting word vectors are saved in a text format file (here) named text8.txt. Import the obtained word vectors from text8.txt into Julia using load_embeddings()","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> model = load_embeddings(\"./text8.txt\")","category":"page"},{"location":"getting_started/#Examples-2","page":"Getting Started","title":"Examples","text":"","category":"section"},{"location":"getting_started/#Functions","page":"Getting Started","title":"Functions","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Now that a model is loaded the functions of this package can be used to work with the embedding vectors.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"get_word2vec(): Retrieves the embedding vector corresponding to a given word.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> get_word2vec(model, \"king\")","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"get_vec2word(): Retrieves the closest word in the embedding space to a given vector.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> get_vec2word(model, king_vec)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"get_vector_operation(): Computes 1 of 4 vector calculations on two input words or vectors depending on the input operator","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> get_vector_operation(model, \"king\", \"queen\",:+)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"or","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> get_vector_operation(model, king_vec, \"queen\",\"euclid\")","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"get_word_analogy(): Performs word analogy calculations (e.g. king - man + woman = queen)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> word_analogy(model, \"king\", \"man\", \"woman\")","category":"page"},{"location":"getting_started/#Display-Data-Functions","page":"Getting Started","title":"Display Data Functions","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"showrelations(): Creates a [PCA Projection](https://en.wikipedia.org/wiki/Principalcomponent_analysis) to 2D of words with connecting vectors ","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> show_relations(\"berlin\", \"germany\", \"paris\", \"france\", \"rome\", \"apple\", wv=model, save_path=\"my_custom_plot.png\")","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"<div align=\"center\">   <img src=\"assets/PCAProjection.png\" alt=\"Logo\" width=\"400\" height=\"250\" /> </div>","category":"page"},{"location":"getting_started/#Train-Model-and-Create-Word-Embeddings-fasttext","page":"Getting Started","title":"Train Model and Create Word Embeddings - fasttext","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"As an alternative use a (larger) text corpus from e.g. FastText (.bin & .vec file) with about 33 million words. Store this file in the current working directory and apply the same functions as in the previous example.","category":"page"},{"location":"getting_started/#For-Developers","page":"Getting Started","title":"For Developers","text":"","category":"section"},{"location":"getting_started/#1)-Download-the-code","page":"Getting Started","title":"1) Download the code","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"git clone https://github.com/graupnej/GroupIWord2Vec.jl.git","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Navigate to the cloned directory and launch julia. Activate the project environment to tell Julia to use the Project.toml","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> using Pkg\njulia> Pkg.activate(\".\")","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Resolve dependencies and create a Manifest.toml file","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> Pkg.instantiate()","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Precompile the project to ensure all dependencies and the code is ready","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> Pkg.precompile()","category":"page"},{"location":"getting_started/#2)-Run-tests","page":"Getting Started","title":"2) Run tests","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To verify everything is working correctly run the code coverage tests","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> Pkg.test(\"GroupIWord2Vec\")","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"This covers all the tests. To execute a specific test (e.g. Functions)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> Pkg.test(\"GroupIWord2Vec\", test_args=[\"Functions\"])","category":"page"},{"location":"getting_started/#Dependencies","page":"Getting Started","title":"Dependencies","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The package relies on the following non-standard Julia packages:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"   DelimitedFiles        # Provides functionality for reading and writing delimited text files\n   LinearAlgebra         # Offers a suite of mathematical tools and operations for linear algebra\n   Plots                 # For visualization functions\n   Word2vec.jll          # Links to the underlying Word2Vec implementation (C code)\n   Statistics            # For basic statistical operations (mean, std, var, etc.)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The files Project.toml and Manifest.toml in the created environment manage dependencies.","category":"page"},{"location":"getting_started/#References","page":"Getting Started","title":"References","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The text corpus for the simple example (text8) is a preprocessed version of the first 100 million bytes of the English Wikipedia dump from March 3, 2006. It has been filtered to include only lowercase letters (a–z) and spaces, reducing the dataset's size to approximately 100 MB. It is commonly used for training and evaluating language models.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The text corpus for the large example were obtained using the skip-gram model described in Bojanowski et al. (2016) with default parameters.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"   P. Bojanowski*, E. Grave*, A. Joulin, T. Mikolov, Enriching Word Vectors with Subword Information","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"As inspiration on how to properly develop julia packages, organize source files, write meaningful tests and more read here.","category":"page"},{"location":"getting_started/#Contributors","page":"Getting Started","title":"Contributors","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"(Image: Contributors)","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = GroupIWord2Vec","category":"page"},{"location":"","page":"Home","title":"Home","text":"<div align=\"center\">   <img src=\"assets/WordEmbeddings.png\" alt=\"Logo\" width=\"250\" height=\"250\" />   <h1>Word2Vec</h1>   A Julia package that provides tools for running word embedding algorithms.   <br/>   <a href=\"https://julialang.org/downloads/\">     <img src=\"https://img.shields.io/badge/Julia-v1.10-blue\" alt=\"Julia Version\"/>   </a> </div>","category":"page"},{"location":"#Prerequisities-and-dependencies","page":"Home","title":"Prerequisities and dependencies","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package uses (Image: Julia) and relies on the following non-standard Julia packages:","category":"page"},{"location":"","page":"Home","title":"Home","text":"   DelimitedFiles        # Provides functionality for reading and writing delimited text files\n   LinearAlgebra         # Offers a suite of mathematical tools and operations for linear algebra\n   Plots                 # For visualization functions\n   Word2vec.jll          # Links to the underlying Word2Vec implementation (C code)\n   Statistics            # For basic statistical operations (mean, std, var, etc.)","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"}]
}
