var documenterSearchIndex = {"docs":
[{"location":"functions/","page":"All Functions List","title":"All Functions List","text":"Modules = [GroupIWord2Vec]","category":"page"},{"location":"functions/#GroupIWord2Vec.WordEmbedding","page":"All Functions List","title":"GroupIWord2Vec.WordEmbedding","text":"WordEmbedding\n\nA structure for storing and managing word embeddings, where each word is associated with a vector representation.\n\nFields\n\nwords::Vector{String}: List of all words in the vocabulary\nembeddings::Matrix{Float64}: Matrix where each column is a word's vector representation\nword_indices::Dict{String, Int}: Dictionary mapping words to their positions in the vocabulary\n\nConstructor\n\nWordEmbedding(words::Vector{String}, matrix::Matrix{Float64})\n\nCreates a WordEmbedding with the given vocabulary and corresponding vectors.\n\nThrows\n\nArgumentError: If the number of words doesn't match the number of vectors (matrix columns)\n\nExample\n\n# Create a simple word embedding with 2D vectors\nwords = [\"cat\", \"dog\", \"house\"]\nvectors = [0.5 0.1 0.8;\n          0.2 0.9 0.3]\nembedding = WordEmbedding(words, vectors)\n\n\n\n\n\n","category":"type"},{"location":"functions/#GroupIWord2Vec.create_custom_model-Tuple{Int64, Int64}","page":"All Functions List","title":"GroupIWord2Vec.create_custom_model","text":"create_custom_model(embedding_dim::Int, vocabulary_lenght::Int)::Chain\n\nCreates a Flux model for CBOW.\n\nArguments\n\nembedding_dim::Int: The wanted dimensionality of the embedding. 10-300 is recommended depending on the complexity and resources.\nvocabulary_lenght::Int: Number of words in the vocabulary\n\nReturns\n\nChain: A Flux chain with softmax output\n\nNotes\n\nChain can be used like this my model([2, 5, 18 12]) -> returns prediction of word with the context [2, 5, 18 12] as Softmax probability.  \n\nExample\n\nmy_model = create_custom_model(50, length(my_vocabulary)) \n\n\n\n\n\n","category":"method"},{"location":"functions/#GroupIWord2Vec.create_vocabulary-Tuple{String}","page":"All Functions List","title":"GroupIWord2Vec.create_vocabulary","text":"create_vocabulary(path::String)::Dict{String, Int}\n\nCreates a vocabulary from a textfile with all occuring words.\n\nArguments\n\npath::String: Path to the textfile as string\n\nReturns\n\nDict{String, Int}: A dictionary with the words and coresponding indices\n\nExample\n\nmy_vocabulary = create_vocabulary(\"data/mydataset.txt\")\n\n\n\n\n\n","category":"method"},{"location":"functions/#GroupIWord2Vec.get_any2vec-Tuple{WordEmbedding, Union{String, Vector{Float64}}}","page":"All Functions List","title":"GroupIWord2Vec.get_any2vec","text":"get_any2vec(wv::WordEmbedding, word_or_vec::Union{String, Vector{Float64}}) -> Vector{Float64}\n\nConverts a word into its corresponding vector representation or returns the vector unchanged if already provided\n\nArguments\n\nwv::WordEmbedding: The word embedding structure containing the vocabulary and embeddings\nword_or_vec::Union{String, Vector{Float64}}: A word to be converted into a vector, or a numerical vector to be validated\n\nReturns\n\nVector{Float64}: The vector representation of the word if input is a String, or the validated vector\n\nThrows\n\nDimensionMismatch: If the input vector does not match the embedding dimension.\nArgumentError: If the input is neither a word nor a valid numeric vector.\n\nExample\n\nwords = [\"cat\", \"dog\"]\nvectors = [0.5 0.1;\n          0.2 0.9]\nwv = WordEmbedding(words, vectors)\n\nget_any2vec(wv, \"cat\")  # Returns [0.5, 0.2]\nget_any2vec(wv, [0.5, 0.2])  # Returns [0.5, 0.2]\n\n\n\n\n\n","category":"method"},{"location":"functions/#GroupIWord2Vec.get_similar_words","page":"All Functions List","title":"GroupIWord2Vec.get_similar_words","text":"get_similar_words(wv::WordEmbedding, word_or_vec::Union{AbstractString, AbstractVector{<:Real}}, n::Int=10) -> Vector{String}\n\nFinds the n most similar words to a given word or vector based on cosine similarity.\n\nArguments\n\nwv: The word embedding model.\nword_or_vec: The target word or embedding vector.\nn: Number of similar words to return (default: 10).\n\nThrows\n\nArgumentError: If n is not positive, the word is missing, or the vector has zero norm.\nDimensionMismatch: If the vector size is incorrect.\n\nReturns\n\nA list of n most similar words, sorted by similarity.\n\nExample\n\nget_similar_words(model, \"cat\", 5)  # [\"dog\", \"kitten\", \"feline\", \"puppy\", \"pet\"]\nget_similar_words(model, get_word2vec(model, \"ocean\"), 3)  # [\"sea\", \"water\", \"wave\"]\n\n\n\n\n\n","category":"function"},{"location":"functions/#GroupIWord2Vec.get_vec2word-Tuple{WordEmbedding, Vector{Float64}}","page":"All Functions List","title":"GroupIWord2Vec.get_vec2word","text":"get_vec2word(wv::WordEmbedding,vec::Vector{Float64}) -> String\n\nRetrieves the closest word in the embedding space to a given vector based on cosine similarity.\n\nArguments\n\nwv::WordEmbedding: The word embedding structure containing the vocabulary and embeddings\nvec::Vector{Float64}: A vector representation of a word\n\nReturns\n\nString: The word from the vocabulary closest to the given vector\n\nThrows\n\nDimensionMismatch: If the input vector's dimension does not match the word vector dimensions\n\nExample\n\nwords = [\"cat\", \"dog\"]\nvectors = [0.5 0.1;\n          0.2 0.9]\nembedding = WordEmbedding(words, vectors)\n\nget_vec2word(embedding, [0.51, 0.19])  # Returns \"cat\"\n\n\n\n\n\n","category":"method"},{"location":"functions/#GroupIWord2Vec.get_vector_operation-Tuple{WordEmbedding, Union{String, Vector{Float64}}, Union{String, Vector{Float64}}, Symbol}","page":"All Functions List","title":"GroupIWord2Vec.get_vector_operation","text":"get_vector_operation(ww::WordEmbedding, inp1::Union{String, Vector{Float64}}, inp2::Union{String, Vector{Float64}}, operator::Symbol) -> Union{Vector{Float64}, Float64}\n\nPerforms a mathematical operation between two word embedding vectors\n\nArguments\n\nww::WordEmbedding: The word embedding structure containing the vocabulary and embeddings\ninp1::Union{String, Vector{Float64}}: The first input, which can be a word (String) or a precomputed embedding vector\ninp2::Union{String, Vector{Float64}}: The second input, which can be a word (String) or a precomputed embedding vector\noperator::Symbol: The operation to perform. Must be one of :+, :-, :cosine, or :euclid\n\nThrows\n\nArgumentError: If the operator is invalid.\nArgumentError: If cosine similarity is attempted on a zero vector\nDimensionMismatch: If the input vectors do not have the same length\n\nReturns\n\nVector{Float64}: If the operation is :+ or :-, returns the resulting word vector\nFloat64: If the operation is :cosine or :euclid, returns a scalar value\n\nExample\n\nvec = get_vector_operation(model, \"king\", \"man\", :-)\nsimilarity = get_vector_operation(model, \"cat\", \"dog\", :cosine)\ndistance = get_vector_operation(model, \"car\", \"bicycle\", :euclid)\n\n\n\n\n\n","category":"method"},{"location":"functions/#GroupIWord2Vec.get_word2vec-Tuple{WordEmbedding, String}","page":"All Functions List","title":"GroupIWord2Vec.get_word2vec","text":"get_word2vec(wv::WordEmbedding, word::String) -> Vector{Float64}\n\nRetrieves the embedding vector corresponding to a given word.\n\nArguments\n\nwv::WordEmbedding: The word embedding structure containing the vocabulary and embeddings\nword::String: The word to look up\n\nThrows\n\nArgumentError: If the word is not found in the embedding model\n\nReturns\n\nVector{Float64}: The embedding vector of the requested word of type Float64\n\nExample\n\nvec = get_word2vec(model, \"dog\")\n\n\n\n\n\n","category":"method"},{"location":"functions/#GroupIWord2Vec.get_word_analogy","page":"All Functions List","title":"GroupIWord2Vec.get_word_analogy","text":"get_word_analogy(wv::WordEmbedding, inp1::T, inp2::T, inp3::T, n::Int=5) where {T<:Union{AbstractString, AbstractVector{<:Real}}} -> Vector{String}\n\nFinds the top n words that best complete the analogy: inp1 - inp2 + inp3 = ?.\n\nArguments\n\nwv::WordEmbedding: The word embedding model.\ninp1, inp2, inp3::T: Words or vectors for analogy computation.\nn::Int=5: Number of closest matching words to return.\n\nReturns\n\nVector{String}: A list of the top n matching words.\n\nNotes\n\nInput words are converted to vectors automatically.\nThe computed analogy vector is normalized.\nInput words (if given as strings) are excluded from results.\n\nExample\n\nget_word_analogy(model, \"king\", \"man\", \"woman\", 3) \n# → [\"queen\", \"princess\", \"duchess\"]\n\n\n\n\n\n","category":"function"},{"location":"functions/#GroupIWord2Vec.load_embeddings-Tuple{String}","page":"All Functions List","title":"GroupIWord2Vec.load_embeddings","text":"load_embeddings(path::String; format::Symbol=:text, data_type::Type{Float64}=Float64, separator::Char=' ', skip_bytes::Int=1)\n\nLoads word embeddings from a text or binary file.\n\nArguments\n\npath::String: Path to the embedding file\nformat::Union{:text, :binary}=:text: File format (:text or :binary)\ndata_type::Type{Float64}=Float64: Type of word vectors\nseparator::Char=' ': Word-vector separator in text files\nskip_bytes::Int=0: Bytes to skip after each word-vector pair in binary files\n\nThrows\n\nArgumentError: If format is not :text or :binary\n\nReturns\n\nWordEmbedding: The loaded word embeddings structure\n\nExample\n\nembedding = load_embeddings(\"vectors.txt\")  # Load text format\nembedding = load_embeddings(\"vectors.bin\", format=:binary, data_type=Float64, skip_bytes=1)  # Load binary format\n\n\n\n\n\n","category":"method"},{"location":"functions/#GroupIWord2Vec.read_binary_format-Union{Tuple{T}, Tuple{AbstractString, Type{T}, Bool, Char, Int64}} where T<:Real","page":"All Functions List","title":"GroupIWord2Vec.read_binary_format","text":"\"     readbinaryformat(filepath::AbstractString, ::Type{T}, normalize::Bool,                         separator::Char, skip_bytes::Int) where T<:Real -> WordEmbedding\n\nReads word embeddings from a binary file and converts them into a WordEmbedding object.\n\nArguments\n\nfilepath::AbstractString: Path to the binary file containing word embeddings.\nT<:Real: Numeric type for storing embedding values (e.g., Float32, Float64).\nnormalize::Bool: Whether to normalize vectors to unit length for comparison.\nseparator::Char: Character separating words and vector data in the file.\nskip_bytes::Int: Number of bytes to skip after each word-vector pair (e.g., for handling separators).\n\nThrows\n\nSystemError: If the file cannot be opened or read.\nArgumentError: If the file format is incorrect or data is missing.\n\nReturns\n\nWordEmbedding: A structure containing words and their corresponding embedding vectors.\n\nExample\n\nembeddings = read_binary_format(\"vectors.bin\", Float32, true, ' ', 1)\n\n\n\n\n\n","category":"method"},{"location":"functions/#GroupIWord2Vec.read_text_format-Union{Tuple{T}, Tuple{AbstractString, Type{T}, Bool, Char}} where T<:Real","page":"All Functions List","title":"GroupIWord2Vec.read_text_format","text":"read_text_format(filepath::AbstractString, ::Type{T}, normalize::Bool, \n                 separator::Char) where T<:Real -> WordEmbedding\n\nReads word embeddings from a text file and converts them into a WordEmbedding object.\n\nArguments\n\nfilepath::AbstractString: Path to the text file containing word embeddings.\nT<:Real: Numeric type for storing embedding values (e.g., Float32, Float64).\nnormalize::Bool: Whether to normalize vectors to unit length for comparison.\nseparator::Char: Character used to separate words and vector values in the file.\n\nThrows\n\nSystemError: If the file cannot be opened or read.\nArgumentError: If the file format is incorrect or missing data.\n\nReturns\n\nWordEmbedding: A structure containing words and their corresponding embedding vectors.\n\nExample\n\nembeddings = read_text_format(\"vectors.txt\", Float32, true, ' ')\n\n\n\n\n\n","category":"method"},{"location":"functions/#GroupIWord2Vec.reduce_to_2d","page":"All Functions List","title":"GroupIWord2Vec.reduce_to_2d","text":"reduce_to_2d(data::Matrix{Float64}, number_of_pc::Int=2) -> Matrix{Float64}\n\nPerforms Principal Component Analysis (PCA) to reduce the dimensionality of a given dataset and returns a projected data\n\nArguments\n\ndata::Matrix{Float64}: The input data matrix where rows represent samples and columns represent features.\nnumber_of_pc::Int=2: The number of principal components to retain (default: 2).\n\nReturns\n\nMatrix{Float64}: A matrix of shape (number_of_pc × N), where N is the number of samples, containing the projected data in the reduced dimensional space.\n\nExample\n\ndata = randn(100, 50)  # 100 samples, 50 features\nreduced_data = reduce_to_2d(data, 2)\n\n\n\n\n\n","category":"function"},{"location":"functions/#GroupIWord2Vec.save_custom_model-Tuple{Flux.Chain, Dict{String, Int64}, String}","page":"All Functions List","title":"GroupIWord2Vec.save_custom_model","text":"save_custom_model(model::Chain, vocabulary::Dict{String, Int}, path::String)\n\nSaves the model as a txt in the format for load_embeddings().\n\nArguments\n\nmodel::Chain: The Flux chain from create_model.\nvocabulary::Dict: The vocabulary from create_vocabulary()\npath::String: Path to the file for saving. \n\nNotes\n\nMake sure to choose a file with a .txt ending if you plan to use it with load_embeddings(). \n\nExample\n\nsave_custom_model(my_model, my_vocabulary, \"data/saved_embedd.txt\")\n\n\n\n\n\n","category":"method"},{"location":"functions/#GroupIWord2Vec.sequence_text-Tuple{String, Dict{String, Int64}}","page":"All Functions List","title":"GroupIWord2Vec.sequence_text","text":"sequence_text(path::String, vocabulary::Dict{String, Int})::Vector{Int64}\n\nTransforms a text to a vector of indices that match the words in the vocabulary.\n\nArguments\n\npath::String: Path to the textfile as string\nvocabulary::Dict{String, Int}: Vocabulary as a look up table\n\nReturns\n\nVector{Int64}: A vector of Integers that contains the text in index form eg: [1, 5, 23, 99, 69, ...]\n\nExample\n\nsequence = sequence_text(\"data/mydataset.txt\", my_vocabulary)\n\n\n\n\n\n","category":"method"},{"location":"functions/#GroupIWord2Vec.show_relations-Tuple{Vararg{String}}","page":"All Functions List","title":"GroupIWord2Vec.show_relations","text":"show_relations(words::String...; wv::WordEmbedding, save_path::String=\"word_relations.png\") -> Plots.Plot\n\nGenerates a 2D PCA projection of the given word embeddings and visualizes their relationships like this: arg1==>arg2, arg3==>arg4, ... Note: Use an even number of inputs!\n\nArguments\n\nwords::String...: A list of words to visualize. The number of words must be a multiple of 2.\nwv::WordEmbedding: The word embedding structure containing the word vectors.\nsave_path::String=\"word_relations.png\": The file path for the generated plot. Not saved if empty or nothing\n\nThrows\n\nArgumentError: If the number of words is not a multiple of 2.\nArgumentError: If any of the provided words are not found in the embedding model.\n\nReturns\n\nPlots.Plot: A scatter plot with arrows representing word relationships.\n\nExample\n\np = show_relations(\"king\", \"queen\", \"man\", \"woman\"; wv=model, save_path=\"relations.png\")\n\n\n\n\n\n","category":"method"},{"location":"functions/#GroupIWord2Vec.train_custom_model-Tuple{Flux.Chain, String, Dict{String, Int64}, Int64, Int64}","page":"All Functions List","title":"GroupIWord2Vec.train_custom_model","text":"train_custom_model(model::Chain, dataset::String, vocabulary::Dict ,epochs::Int, window_size::Int; optimizer=Descent(), batchsize=10)::Chain\n\nTrains a model on a given dataset. \n\nArguments\n\nmodel::Chain: The Flux chain from create_model.\ndataset::String: Path to the dataset. \nvocabulary::Dict: The vocabulary from create_vocabulary()\nepochs::Int: Number of desired epochs.\nwindow_size::Int: Window size for the context window. The total window is 2*window size because preceding and following words are used as context.  \noptimizer=Descent(): Optimizer from Flux used for training \nbatchsize=10: Number of words trained per epoch. If batchsize = 0 all words in dataset get used once per epoch.\n\nReturns\n\nChain: The updated Flux Chain after training.\n\nNotes\n\nNumber of words Trained = epchs*batchsize.\n\nExample\n\nmy_updated_model = train_custom_model(my_model, \"data/my_dataset.txt\", my_vocabulary, 10, 1)\n\n\n\n\n\n","category":"method"},{"location":"functions/#GroupIWord2Vec.train_model-Tuple{AbstractString, AbstractString}","page":"All Functions List","title":"GroupIWord2Vec.train_model","text":"train_model(train::AbstractString, output::AbstractString; \n            size::Int=100, window::Int=5, sample::AbstractFloat=1e-3,\n            hs::Int=0, negative::Int=5, threads::Int=12, iter::Int=5, \n            min_count::Int=5, alpha::AbstractFloat=0.025,\n            debug::Int=2, binary::Int=0, cbow::Int=1, \n            save_vocab=Nothing(), read_vocab=Nothing(),\n            verbose::Bool=false) -> Nothing\n\nTrains a Word2Vec model using the specified parameters.\n\nArguments\n\ntrain::AbstractString: Path to the input text file used for training.\noutput::AbstractString: Path to save the trained word vectors.\nsize::Int: Dimensionality of the word vectors (default: 100).\nwindow::Int: Maximum skip length between words (default: 5).\nsample::AbstractFloat: Threshold for word occurrence downsampling (default: 1e-3).\nhs::Int: Use hierarchical softmax (1 = enabled, 0 = disabled, default: 0).\nnegative::Int: Number of negative samples (0 = disabled, common values: 5-10, default: 5).\nthreads::Int: Number of threads for training (default: 12).\niter::Int: Number of training iterations (default: 5).\nmin_count::Int: Minimum occurrences for a word to be included (default: 5).\nalpha::AbstractFloat: Initial learning rate (default: 0.025).\ndebug::Int: Debugging verbosity level (default: 2).\nbinary::Int: Save the vectors in binary format (1 = enabled, 0 = disabled, default: 0).\ncbow::Int: Use continuous bag-of-words model (1 = CBOW, 0 = Skip-gram, default: 1).\nsave_vocab: Path to save the vocabulary (default: Nothing()).\nread_vocab: Path to read an existing vocabulary (default: Nothing()).\nverbose::Bool: Print training progress (default: false).\n\nThrows\n\nSystemError: If the training process encounters an issue with file paths.\nArgumentError: If input parameters are invalid.\n\nReturns\n\nNothing: The function trains the model and saves the output to a file.\n\nExample\n\ntrain_model(\"data.txt\", \"model.vec\"; size=200, window=10, iter=10)\n\n\n\n\n\n","category":"method"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"(Image: Coverage) (Image: Build Status) (Image: Dev)","category":"page"},{"location":"getting_started/#Word-Embeddings","page":"Getting Started","title":"Word Embeddings","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Word Embeddings are numerical representations of words in a high-dimensional vector space, where words with similar meanings are positioned closer together. These vectors capture semantic relationships between words, allowing machines to understand language context and meaning through mathematical operations. They serve as the foundation for many natural language processing tasks. This package allows training an ML model to create word embeddings based on a source text and provides functionality to work with the generated word embedding vectors.","category":"page"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"This is an overview of the project's main file structure:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"GroupIWord2Vec.jl               \n├── src/                        # Contains core modules for the package\n│   ├── GroupIWord2Vec.jl       # Main entry point for the project\n│   ├── functions.jl            # Word/vector functions\n│   └── model.jl                # Model functions\n├── test/                       # Unit tests to validate functionalities\n│   ├── runtests.jl             # Combination of every testing routine\n│   ├── test_functions.jl       # Testing routine for word/vector functions \n│   └── test_model.jl           # Testing routine for model functions\n├── docs/                       # Documentation for the package\n├── Manifest.toml               # Detailed dependency lock file that tracks exact versions of project dependencies\n├── Project.toml                # Project configuration file defining package dependencies\n└── README.md                   # Main documentation file containing getting started","category":"page"},{"location":"getting_started/#Download","page":"Getting Started","title":"Download","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Pluto's built-in environments cannot be used, so to start a custom environment must be created and managed manually.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> using Pkg\njulia> Pkg.activate(\"MyEnv\")\njulia> Pkg.add(url=\"https://github.com/graupnej/GroupIWord2Vec.jl\")\njulia> using GroupIWord2Vec","category":"page"},{"location":"getting_started/#Examples","page":"Getting Started","title":"Examples","text":"","category":"section"},{"location":"getting_started/#Train-Model-and-Create-Word-Embeddings-Text8","page":"Getting Started","title":"Train Model and Create Word Embeddings - Text8","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Download the text corpus text8 and store it in the current working directory. To train the model with this text corpus use train_model()","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> train_model(\"text8\", \"text8.txt\", verbose = true)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The resulting word vectors are saved in a text format file (here) named text8.txt. Import the obtained word vectors from text8.txt into Julia using load_embeddings()","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> model = load_embeddings(\"./text8.txt\")","category":"page"},{"location":"getting_started/#Examples-2","page":"Getting Started","title":"Examples","text":"","category":"section"},{"location":"getting_started/#Functions","page":"Getting Started","title":"Functions","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Now that a model is loaded the functions of this package can be used to work with the embedding vectors.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"get_word2vec(): Retrieves the embedding vector corresponding to a given word.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> get_word2vec(model, \"king\")","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"get_vec2word(): Retrieves the closest word in the embedding space to a given vector.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> get_vec2word(model, king_vec)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"get_vector_operation(): Computes 1 of 4 vector calculations on two input words or vectors depending on the input operator","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> get_vector_operation(model, \"king\", \"queen\",:+)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"or","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> get_vector_operation(model, king_vec, \"queen\",\"euclid\")","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"get_word_analogy(): Performs word analogy calculations (e.g. king - man + woman = queen)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> word_analogy(model, \"king\", \"man\", \"woman\")","category":"page"},{"location":"getting_started/#Display-Data-Functions","page":"Getting Started","title":"Display Data Functions","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"show_relations(): Creates a PCA Projection to 2D of words with connecting vectors ","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> show_relations(\"berlin\", \"germany\", \"paris\", \"france\", \"rome\", \"apple\", wv=model, save_path=\"my_custom_plot.png\")","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"<div align=\"center\">   <img src=\"assets/PCAProjection.png\" alt=\"Logo\" width=\"400\" height=\"250\" /> </div>","category":"page"},{"location":"getting_started/#Train-Model-and-Create-Word-Embeddings-fasttext","page":"Getting Started","title":"Train Model and Create Word Embeddings - fasttext","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"As an alternative use a (larger) text corpus from e.g. FastText (.bin & .vec file) with about 33 million words. Store this file in the current working directory and apply the same functions as in the previous example.","category":"page"},{"location":"getting_started/#For-Developers","page":"Getting Started","title":"For Developers","text":"","category":"section"},{"location":"getting_started/#1)-Download-the-code","page":"Getting Started","title":"1) Download the code","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"git clone https://github.com/graupnej/GroupIWord2Vec.jl.git","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Navigate to the cloned directory and launch julia. Activate the project environment to tell Julia to use the Project.toml","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> using Pkg\njulia> Pkg.activate(\".\")","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Resolve dependencies and create a Manifest.toml file","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> Pkg.instantiate()","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Precompile the project to ensure all dependencies and the code is ready","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> Pkg.precompile()","category":"page"},{"location":"getting_started/#2)-Run-tests","page":"Getting Started","title":"2) Run tests","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To verify everything is working correctly run the code coverage tests","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> Pkg.test(\"GroupIWord2Vec\")","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"This covers all the tests. To execute a specific test (e.g. Functions)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> Pkg.test(\"GroupIWord2Vec\", test_args=[\"Functions\"])","category":"page"},{"location":"getting_started/#Dependencies","page":"Getting Started","title":"Dependencies","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The package relies on the following non-standard Julia packages:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"   DelimitedFiles        # Provides functionality for reading and writing delimited text files\n   LinearAlgebra         # Offers a suite of mathematical tools and operations for linear algebra\n   Plots                 # For visualization functions\n   Word2vec.jll          # Links to the underlying Word2Vec implementation (C code)\n   Statistics            # For basic statistical operations (mean, std, var, etc.)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The files Project.toml and Manifest.toml in the created environment manage dependencies.","category":"page"},{"location":"getting_started/#References","page":"Getting Started","title":"References","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The text corpus for the simple example (text8) is a preprocessed version of the first 100 million bytes of the English Wikipedia dump from March 3, 2006. It has been filtered to include only lowercase letters (a–z) and spaces, reducing the dataset's size to approximately 100 MB. It is commonly used for training and evaluating language models.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The text corpus for the large example were obtained using the skip-gram model described in Bojanowski et al. (2016) with default parameters.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"   P. Bojanowski*, E. Grave*, A. Joulin, T. Mikolov, Enriching Word Vectors with Subword Information","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"As inspiration on how to properly develop julia packages, organize source files, write meaningful tests and more read here.","category":"page"},{"location":"getting_started/#Contributors","page":"Getting Started","title":"Contributors","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"(Image: Contributors)","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = GroupIWord2Vec","category":"page"},{"location":"","page":"Home","title":"Home","text":"<div align=\"center\">   <img src=\"assets/WordEmbeddings.png\" alt=\"Logo\" width=\"250\" height=\"250\" />   <h1>Word2Vec</h1>   A Julia package that provides tools for running word embedding algorithms.   <br/>   <a href=\"https://julialang.org/downloads/\">     <img src=\"https://img.shields.io/badge/Julia-v1.10-blue\" alt=\"Julia Version\"/>   </a> </div>","category":"page"},{"location":"#Prerequisities-and-dependencies","page":"Home","title":"Prerequisities and dependencies","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package uses (Image: Julia) and relies on the following non-standard Julia packages:","category":"page"},{"location":"","page":"Home","title":"Home","text":"   DelimitedFiles        # Provides functionality for reading and writing delimited text files\n   LinearAlgebra         # Offers a suite of mathematical tools and operations for linear algebra\n   Plots                 # For visualization functions\n   Word2vec.jll          # Links to the underlying Word2Vec implementation (C code)\n   Statistics            # For basic statistical operations (mean, std, var, etc.)","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"}]
}
